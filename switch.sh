#!/bin/bash

# Enable strict error handling for safer script execution
set -euo pipefail

# --- Configuration ---
# Source common configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
if [ -f "$SCRIPT_DIR/config.sh" ]; then
    source "$SCRIPT_DIR/config.sh"
else
    # Fallback values if config.sh is not found
    POD_NETWORK_CIDR="10.244.0.0/16"  # Default for Calico
fi

# Define the standard location for the kubeconfig file generated by kubeadm
KUBECONFIG_PATH="/etc/kubernetes/admin.conf"
# Define the standard directory for static pod manifests
K8S_MANIFESTS_DIR="/etc/kubernetes/manifests"
# Define potential runtime socket paths (add others if needed)
CONTAINERD_SOCK="/run/containerd/containerd.sock"
# Add other runtime sockets if necessary, e.g., DOCKER_SOCK="/var/run/docker.sock"

# Kubeadm init configuration
# POD_NETWORK_CIDR is now loaded from config.sh

# Log files
INIT_LOG="kubeadm-init.log"
RESET_LOG="kubeadm-reset.log"

# --- Terminal Colors ---
RESET='\033[0m'
BOLD='\033[1m'
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
CYAN='\033[0;36m'
MAGENTA='\033[0;35m'

# --- Helper Functions ---
log_step() { echo -e "\n${MAGENTA}${BOLD}==> $1${RESET}"; }
log_info() { echo -e "${CYAN}INFO:${RESET} $1"; }
log_warn() { echo -e "${YELLOW}WARN:${RESET} $1"; }
log_success() { echo -e "${GREEN}SUCCESS:${RESET} $1"; }
error_exit() { echo -e "\n${RED}${BOLD}ERROR:${RESET}${RED} $1${RESET}\n" >&2; exit 1; }

check_root() {
  if [ "$(id -u)" -ne 0 ]; then
    error_exit "This script must be run as root (e.g., using sudo)."
  fi
}

# Function to check if a command exists
check_command() {
  if ! command -v "$1" &> /dev/null; then
    error_exit "Required command '${BOLD}$1${RESET}${RED}' not found. Please ensure Kubernetes components (kubeadm, kubelet, kubectl) and a container runtime (like containerd) are installed."
  fi
}

# Function to check if the cluster appears to be initialized
is_cluster_initialized() {
  [ -f "$KUBECONFIG_PATH" ] || [ -d "$K8S_MANIFESTS_DIR" ]
}

# Function to check if the container runtime socket exists and service is active
# Returns 0 if ready, 1 if socket missing, 2 if service inactive
is_runtime_ready() {
    local runtime_active=false
    if [ -S "$CONTAINERD_SOCK" ]; then
        # Check if service is active too
        if systemctl is-active --quiet containerd; then
            runtime_active=true
        else
           log_warn "Containerd socket found ($CONTAINERD_SOCK), but service is inactive."
           return 2 # Service inactive
        fi
    # Add elif checks for other runtimes if needed
    # elif [ -S "$DOCKER_SOCK" ]; then ...
    fi

    if $runtime_active; then
        log_info "Container runtime (containerd) is active and socket found."
        return 0 # Ready
    else
        log_info "Container runtime (containerd) socket not found or service inactive."
        return 1 # Socket missing or service inactive (treats as not ready)
    fi
}

# Function to check if Kubelet is active
is_kubelet_active() {
    systemctl is-active --quiet kubelet
}

# --- Action Functions ---

start_cluster() {
  log_step "Attempting to start Kubernetes cluster..."
  check_command kubeadm
  check_command kubelet
  check_command kubectl
  check_command systemctl
  check_command modprobe
  check_command sysctl
  check_command swapoff
  check_command sed

  # Check runtime state first
  runtime_status=0
  is_runtime_ready || runtime_status=$?

  if [ $runtime_status -ne 0 ]; then
       log_info "Attempting to start containerd service..."
       systemctl start containerd
       sleep 3
       is_runtime_ready || error_exit "Container runtime (containerd) failed to start or socket not found. Cannot proceed."
       log_success "Containerd service started."
  fi

  if is_cluster_initialized; then
    log_info "State: Existing Kubernetes configuration found."
    export KUBECONFIG="$KUBECONFIG_PATH" # Export for potential kubectl use later
    if is_kubelet_active; then
      log_success "State: Kubelet service is already active. Cluster should be running."
      log_info "Verifying node status..."
      if kubectl get nodes > /dev/null 2>&1; then
          kubectl get nodes
      else
          log_warn "Cannot connect to API server, even though Kubelet is active. Please investigate ('kubectl get nodes', 'journalctl -u kubelet')."
      fi
    else
      log_info "State: Kubelet service is not active."
      log_info "Action: Starting Kubelet service..."
      systemctl start kubelet
      sleep 5 # Give Kubelet time to start
      if is_kubelet_active; then
        log_success "Kubelet service started successfully."
        log_info "Waiting briefly for node registration..."
        sleep 10
        if kubectl get nodes > /dev/null 2>&1; then
           kubectl get nodes
        else
           log_warn "Could not get node status after starting Kubelet. Cluster might still be initializing or have issues."
        fi
      else
        error_exit "Failed to start Kubelet service. Check logs with 'journalctl -u kubelet'."
      fi
    fi
  else
    log_info "State: No existing Kubernetes configuration found."
    if is_kubelet_active; then
        log_warn "Kubelet service is running, but no cluster configuration ($KUBECONFIG_PATH) found. This is unusual."
        read -p "$(echo -e "${YELLOW}Stop the running Kubelet before initializing? [y/N]: ${RESET}")" stop_kubelet
        if [[ "$stop_kubelet" =~ ^[Yy]$ ]]; then
            log_info "Stopping Kubelet..."
            systemctl stop kubelet
            sleep 2
            if is_kubelet_active; then error_exit "Failed to stop Kubelet. Cannot proceed with initialization."; fi
        else
            error_exit "Cannot initialize cluster while Kubelet is running without configuration. Please stop it manually or allow the script to stop it."
        fi
    fi

    log_info "Action: Initializing a new cluster..."

    # --- Pre-flight Checks ---
    log_info "Running pre-flight checks..."
    # 1. Disable Swap
    if grep -q " swap " /proc/swaps; then
      log_info "- Disabling swap..."
      swapoff -a
      # More specific sed: only comments out lines starting without # and containing ' swap '
      sed -i.bak -E 's|^([^#].*\s+swap\s+.*)$|#\1|' /etc/fstab
      log_info "  Swap disabled. Original fstab line commented out (backup: /etc/fstab.bak)."
    else
      log_info "- Swap already disabled."
    fi

    # 2. Kernel Modules and Parameters
    local modules_changed=false
    if ! lsmod | grep -q "^br_netfilter\s"; then
        log_info "- Loading br_netfilter module..."
        modprobe br_netfilter || log_warn "Could not load br_netfilter module."
        modules_changed=true
    fi
    # Ensure required sysctl settings are applied (kubeadm checks these too)
    log_info "- Applying required sysctl settings (bridge-nf-call-iptables, ip_forward)..."
    cat <<EOF | tee /etc/sysctl.d/99-kubernetes-cri.conf > /dev/null
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
    sysctl --system > /dev/null || log_warn "Could not apply sysctl settings."
    if $modules_changed; then log_info "- Kernel settings updated."; else log_info "- Kernel settings appear correct."; fi
    # --- End Pre-flight ---

    log_info "Action: Running 'kubeadm init' with Pod CIDR ${POD_NETWORK_CIDR}..."
    log_info "(This can take several minutes. Output logged to ${INIT_LOG})"
    # Use --ignore-preflight-errors=Swap because we disabled it above
    kubeadm init --pod-network-cidr="${POD_NETWORK_CIDR}" --ignore-preflight-errors=Swap > "$INIT_LOG" 2>&1
    INIT_EXIT_CODE=$?

    if [ $INIT_EXIT_CODE -ne 0 ]; then
        echo # Newline after potential kubeadm output
        log_warn "'kubeadm init' failed (exit code $INIT_EXIT_CODE)."
        log_warn "Check full output in ${BOLD}${INIT_LOG}${RESET}${YELLOW}."
        # Show last few lines of the log
        tail -n 15 "$INIT_LOG"
        error_exit "Cluster initialization failed."
    fi
    if [ ! -f "$KUBECONFIG_PATH" ]; then
        error_exit "'kubeadm init' seemed to succeed, but '$KUBECONFIG_PATH' was not created. Check ${INIT_LOG}."
    fi
    log_success "Cluster initialized successfully."
    log_info "Raw output available in ${INIT_LOG}."

    log_info "Action: Configuring kubectl access for root user..."
    mkdir -p /root/.kube
    cp -i "$KUBECONFIG_PATH" /root/.kube/config
    chown "$(id -u)":"$(id -g)" /root/.kube/config
    export KUBECONFIG="$KUBECONFIG_PATH" # Use the admin conf directly
    log_success "kubectl configured for root."
    echo -e "${YELLOW}NOTE: For non-root user access, run the following AS THE NON-ROOT USER:\n  mkdir -p \$HOME/.kube\n  sudo cp -i $KUBECONFIG_PATH \$HOME/.kube/config\n  sudo chown \$(id -u):\$(id -g) \$HOME/.kube/config${RESET}"

    log_info "Action: Untainting control-plane node to allow scheduling general pods..."
    # Wait briefly for API server to be fully responsive after init
    sleep 5
    NODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
    if [ -n "$NODE_NAME" ]; then
        log_info "- Found control-plane node: $NODE_NAME"
        if kubectl taint nodes "$NODE_NAME" node-role.kubernetes.io/control-plane:NoSchedule-; then
            log_success "- Node '$NODE_NAME' untainted successfully."
            log_info "- Waiting for node to report Ready status..."
             if kubectl wait --for=condition=Ready node/"$NODE_NAME" --timeout=120s; then
                 log_success "- Node '$NODE_NAME' is Ready."
             else
                 log_warn "- Node '$NODE_NAME' did not become Ready within the timeout."
             fi
        else
            log_warn "- Failed to untaint node '$NODE_NAME'. Pods might not schedule on it."
        fi
    else
        log_warn "- Could not determine control-plane node name automatically to untaint."
    fi

    log_warn "IMPORTANT: Cluster started, but you MUST install a CNI network plugin (e.g., Calico, Flannel) for pods to communicate and CoreDNS to start."
    log_info "Example (Calico): kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.2/manifests/calico.yaml" # Check for latest version
    log_info "Current node status:"
    kubectl get nodes -o wide
  fi
}

stop_cluster() {
  log_step "Attempting to stop Kubernetes cluster services..."
  check_command systemctl

  local kubelet_was_active=false
  local runtime_was_active=false

  if is_kubelet_active; then
    kubelet_was_active=true
    log_info "State: Kubelet service is active."
    log_info "Action: Stopping Kubelet service..."
    systemctl stop kubelet
    sleep 2 # Give it a moment
    if is_kubelet_active; then
      log_warn "Failed to stop Kubelet service. Please check 'journalctl -u kubelet'."
    else
      log_success "Kubelet service stopped."
    fi
  else
    log_info "State: Kubelet service is already inactive."
  fi

  # Check runtime status after potentially stopping kubelet
  is_runtime_ready && runtime_was_active=true

  if $runtime_was_active; then
      read -p "$(echo -e "${CYAN}Stop the container runtime (containerd) as well? [y/N]: ${RESET}")" stop_runtime
      if [[ "$stop_runtime" =~ ^[Yy]$ ]]; then
        log_info "Action: Stopping containerd service..."
        systemctl stop containerd
        sleep 2
        if systemctl is-active --quiet containerd; then
            log_warn "Failed to stop containerd service."
        else
            log_success "Containerd service stopped."
            runtime_was_active=false # Update status for final message
        fi
      else
          log_info "Action: Container runtime (containerd) left running."
      fi
  fi

  if ! $kubelet_was_active && ! $runtime_was_active; then
     log_info "Cluster services were already stopped."
  else
     log_success "Cluster services stopped (Kubelet: $( $kubelet_was_active && echo stopped || echo was inactive), Containerd: $( $runtime_was_active && echo running || echo stopped ))."
  fi
}

destroy_cluster() {
  log_step "Attempting to DESTROY Kubernetes cluster configuration..."
  check_command kubeadm
  check_command systemctl
  check_command sed

  if ! is_cluster_initialized; then
    log_info "State: No existing Kubernetes configuration found ($KUBECONFIG_PATH or $K8S_MANIFESTS_DIR)."
    log_warn "If a cluster exists but uses non-standard paths, manual cleanup is needed."
    exit 0
  fi

  echo # Blank line before warning
  log_warn "${BOLD}This action is DESTRUCTIVE and potentially IRREVERSIBLE.${RESET}"
  log_warn "It will run 'kubeadm reset' and attempt to remove configuration files."
  read -p "$(echo -e "${RED}ARE YOU ABSOLUTELY SURE? Type '${BOLD}yes${RESET}${RED}' to confirm destruction: ${RESET}")" confirmation

  if [[ "$confirmation" != "yes" ]]; then
    log_info "Cluster destruction cancelled."
    exit 0
  fi

  log_info "Action: Running 'kubeadm reset'..."
  log_info "(Output logged to ${RESET_LOG})"
  kubeadm reset --force > "$RESET_LOG" 2>&1
  RESET_EXIT_CODE=$?

  if [ $RESET_EXIT_CODE -ne 0 ]; then
      log_warn "'kubeadm reset' finished with errors (exit code $RESET_EXIT_CODE). See ${RESET_LOG}. Proceeding with cleanup anyway."
      tail -n 10 "$RESET_LOG"
  else
      log_success "'kubeadm reset' completed successfully. Log in ${RESET_LOG}."
  fi

  log_info "Action: Cleaning up common Kubernetes/CNI directories and files..."
  # Stop services again in case reset failed or left them running
  systemctl stop kubelet &>/dev/null || true
  systemctl stop containerd &>/dev/null || true

  # Safely clean up etcd directory - check for mounts first
  if mountpoint -q /var/lib/etcd 2>/dev/null; then
      log_warn "etcd directory appears to be mounted. Attempting to unmount..."
      umount /var/lib/etcd || log_warn "Failed to unmount /var/lib/etcd"
  fi
  
  # Check if etcd directory is still mounted before removal
  if ! mountpoint -q /var/lib/etcd 2>/dev/null; then
      rm -rf /var/lib/etcd
      log_info "- Removed etcd data directory"
  else
      log_warn "etcd directory still mounted - skipping removal for safety"
  fi

  # Clean up other directories
  rm -rf /root/.kube \
         "$KUBECONFIG_PATH" \
         "$K8S_MANIFESTS_DIR" \
         /var/lib/cni/ \
         /var/lib/kubelet/* \
         /etc/cni/net.d \
         /run/flannel/ # Common flannel runtime file
  log_success "- Removed common directories."
  # Optional: ip link delete cni0; ip link delete flannel.1 etc. if needed

  log_info "Action: Attempting to restore swap setting in /etc/fstab (if modified by this script)..."
  if [ -f /etc/fstab.bak ]; then
      # This sed command attempts to uncomment lines previously commented by this script's start command
      sed -i.bak2 -E 's|^#([^#].*\s+swap\s+.*)$|\1|' /etc/fstab
      if ! grep -q " swap " /proc/swaps && grep -E '^[^#].*\s+swap\s+' /etc/fstab; then
          log_info "- Swap entry in /etc/fstab seems restored. Reboot or run 'swapon -a' to re-enable if desired."
          # Clean up intermediate backup if successful
          rm -f /etc/fstab.bak2
      elif grep -q " swap " /proc/swaps; then
           log_info "- Swap is already active, fstab change might not have been needed or didn't take effect without swapon/reboot."
      else
          log_warn "- Could not reliably restore swap entry in /etc/fstab automatically. Original backup: /etc/fstab.bak. Second backup: /etc/fstab.bak2"
      fi
  else
      log_info "- No /etc/fstab.bak found, assuming fstab was not modified by this script."
  fi

  log_success "Kubernetes cluster destruction attempted."
  log_warn "Review system state, check $RESET_LOG, and manually clean any remaining resources (e.g., CNI interfaces, firewall rules) if necessary."
}

show_status() {
    log_step "Checking Kubernetes Cluster Status..."
    check_command systemctl
    check_command kubectl || log_warn "'kubectl' command not found, status check will be limited."

    local runtime_status_msg="Unknown"
    is_runtime_ready >/dev/null 2>&1
    case $? in
      0) runtime_status_msg="${GREEN}Active${RESET}" ;;
      1) runtime_status_msg="${RED}Not Found / Inactive${RESET}" ;;
      2) runtime_status_msg="${YELLOW}Found but Inactive${RESET}" ;;
    esac
    log_info "Container Runtime (containerd): $runtime_status_msg"

    local initialized=false
    if is_cluster_initialized; then
        initialized=true
        log_info "Cluster Configuration: ${GREEN}Initialized${RESET} (Found $KUBECONFIG_PATH or $K8S_MANIFESTS_DIR)"
    else
        log_info "Cluster Configuration: ${YELLOW}Not Initialized${RESET}"
    fi

    local kubelet_active=false
    if is_kubelet_active; then
        kubelet_active=true
        log_info "Kubelet Service: ${GREEN}Active${RESET}"
    else
        log_info "Kubelet Service: ${RED}Inactive${RESET}"
    fi

    if $initialized && $kubelet_active && command -v kubectl &> /dev/null; then
        log_info "Attempting API Server connection..."
        export KUBECONFIG="$KUBECONFIG_PATH"
        if kubectl get nodes > /dev/null 2>&1; then
            log_success "API Server: Reachable"
            echo -e "\n${BOLD}Node Status:${RESET}"
            kubectl get nodes -o wide
            echo -e "\n${BOLD}CoreDNS Status (kube-system):${RESET}" # Basic CNI health check proxy
            kubectl get pods -n kube-system -l k8s-app=kube-dns || kubectl get pods -n kube-system -l k8s-app=coredns || log_warn "Could not get CoreDNS pods."
            # Count pods
            local pod_count
            pod_count=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | wc -l)
            log_info "Total Pods (all namespaces): $pod_count"
        else
            log_warn "API Server: ${RED}Unreachable${RESET} (Kubelet active but 'kubectl get nodes' failed)"
        fi
    elif $initialized && ! $kubelet_active; then
        log_warn "Cluster is initialized but Kubelet is stopped. Use '${BOLD}start${RESET}' action."
    elif ! $initialized; then
        log_info "Cluster is not initialized. Use '${BOLD}start${RESET}' action to initialize."
    fi
     echo # Trailing newline
}


# --- Main Script Logic ---

check_root

# Display usage if no argument is provided
if [ -z "$1" ]; then
  echo "Usage: $0 [start|stop|destroy|status]"
  echo "  start   - Initializes a new cluster if none exists, or starts services if configuration found but stopped."
  echo "           (Requires containerd, kubeadm, kubelet, kubectl)"
  echo "  stop    - Stops the Kubelet service (and optionally containerd)."
  echo "  destroy - Runs 'kubeadm reset', cleans up config files (requires confirmation)."
  echo "  status  - Checks the current state of the cluster services and configuration."
  exit 1
fi

ACTION=$(echo "$1" | tr '[:upper:]' '[:lower:]') # Convert action to lowercase

case "$ACTION" in
  start)
    start_cluster
    ;;
  stop)
    stop_cluster
    ;;
  destroy)
    destroy_cluster
    ;;
  status)
    show_status
    ;;
  *)
    echo -e "${RED}Invalid action: $1${RESET}"
    echo "Usage: $0 [start|stop|destroy|status]"
    exit 1
    ;;
esac

exit 0
